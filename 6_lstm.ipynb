{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        filename, _ = urlretrieve(url + filename, filename)\n",
    "    statinfo = os.stat(filename)\n",
    "    if statinfo.st_size == expected_bytes:\n",
    "        print('Found and verified %s' % filename)\n",
    "    else:\n",
    "        print(statinfo.st_size)\n",
    "        raise Exception(\n",
    "            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "    return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "    with zipfile.ZipFile(filename) as f:\n",
    "        name = f.namelist()[0]\n",
    "        data = tf.compat.as_str(f.read(name))\n",
    "    return data\n",
    "    \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "    if char in string.ascii_lowercase:\n",
    "        return ord(char) - first_letter + 1\n",
    "    elif char == ' ':\n",
    "        return 0\n",
    "    else:\n",
    "        print('Unexpected character: %s' % char)\n",
    "        return 0\n",
    "    \n",
    "def id2char(dictid):\n",
    "    if dictid > 0:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    else:\n",
    "        return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "            self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "    \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "    predictions[predictions < 1e-10] = 1e-10\n",
    "    return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "    \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "    probabilities.\n",
    "    \"\"\"\n",
    "    r = random.uniform(0, 1)\n",
    "    s = 0\n",
    "    for i in range(len(distribution)):\n",
    "        s += distribution[i]\n",
    "        if s >= r:\n",
    "            return i\n",
    "    return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "    \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "    p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "    p[0, sample_distribution(prediction[0])] = 1.0\n",
    "    return p\n",
    "\n",
    "def random_distribution():\n",
    "    \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "    b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "    return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Parameters:\n",
    "    # Input gate: input, previous output, and bias.\n",
    "    ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Forget gate: input, previous output, and bias.\n",
    "    fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Memory cell: input, state and bias.                                                         \n",
    "    cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Output gate: input, previous output, and bias.\n",
    "    ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "    om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "    ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "        forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "        update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.298592 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "================================================================================\n",
      "dewocsk c eplenrkoeardefchldoqcp cmgqm dafcinup qefiahrav h os hroz noo nbte g y\n",
      "innhkxhql mohigoxedramsaenm qqsnfje nzrtdnznhm xjanaizs gnqdlwlp y obgqqhf  uyto\n",
      "hhaie keq qsfsrmi lxwjzjmdkidbpvoc tjaaj tmle sulqaqcttq bpesi tv  e f zfdclhmem\n",
      "nghamdttthrh cyjeczlckhdbaji m psd idevqieab xxj rrspneer pijd   pikpvu gaf cpit\n",
      "w per uerttsraew e bo yhvasqrycsaqw ixmvcpcsvftyn mpn enrqotziwqhjpqaebmtmy khf \n",
      "================================================================================\n",
      "Validation set perplexity: 20.19\n",
      "Average loss at step 100: 2.592570 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.02\n",
      "Validation set perplexity: 10.44\n",
      "Average loss at step 200: 2.245099 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.45\n",
      "Validation set perplexity: 8.41\n",
      "Average loss at step 300: 2.089713 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.42\n",
      "Validation set perplexity: 7.93\n",
      "Average loss at step 400: 1.993087 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Validation set perplexity: 7.70\n",
      "Average loss at step 500: 1.934095 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.65\n",
      "Validation set perplexity: 7.05\n",
      "Average loss at step 600: 1.909299 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.95\n",
      "Average loss at step 700: 1.856823 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.54\n",
      "Validation set perplexity: 6.60\n",
      "Average loss at step 800: 1.819355 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.04\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 900: 1.826819 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.88\n",
      "Validation set perplexity: 6.14\n",
      "Average loss at step 1000: 1.822347 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.69\n",
      "================================================================================\n",
      "copt qumpbets of chs iq was mistify is beet fews ededioum riiqh the liding is ni\n",
      "ing most inpleen atiinn of tont many cribed qutite enowil behak wals beprecelo c\n",
      "ing bettora of the relqeas of sodorg aly sover feathro dives ster lints of mevol\n",
      "cesp the ly be ha strile dsy wre ce the oflem weqzal two fers was and eighneed t\n",
      "miston zerolity prost low as aresdicars proveds of clume of one nine zero monw a\n",
      "================================================================================\n",
      "Validation set perplexity: 6.04\n",
      "Average loss at step 1100: 1.776027 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.88\n",
      "Average loss at step 1200: 1.749882 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.76\n",
      "Average loss at step 1300: 1.735508 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Validation set perplexity: 5.73\n",
      "Average loss at step 1400: 1.741629 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 5.63\n",
      "Average loss at step 1500: 1.737359 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.745966 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.38\n",
      "Average loss at step 1700: 1.711118 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.50\n",
      "Average loss at step 1800: 1.677122 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 1900: 1.647755 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2000: 1.698454 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "================================================================================\n",
      "pringescent on by buicgleanchay one nine zero itsin productional loges constiren\n",
      "jyt jack as porfiens constructions toppocialal by distied excevery aldorkaner my\n",
      "kely is use three one nine nine eight three foundhtive somes lande arry shaping \n",
      "de fooun seith by the last bards shown ubathas k usid abonatious victrical film \n",
      "clivision and apranss studs a schood one nine one two four ensude zero sturwes p\n",
      "================================================================================\n",
      "Validation set perplexity: 5.28\n",
      "Average loss at step 2100: 1.686298 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2200: 1.683736 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.36\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2300: 1.644481 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 2400: 1.660575 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 2500: 1.683927 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 2600: 1.654602 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 2700: 1.657274 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2800: 1.652529 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 2900: 1.653013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3000: 1.651666 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "tifficc of theorizer one nine five firso arable that regra s is with selia compl\n",
      "y a unopulay to relied caute in the will nd nine gissia film or hangn hands with\n",
      "lagitions as and with there capart of a gress an earch calfors the amis d inscol\n",
      "joper and six five six he recoded of his a gall firdcimmbea such yoraptement ds \n",
      "x that rizs goll one nine seven patcleth refinctly armine saits upted and sharte\n",
      "================================================================================\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3100: 1.630715 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3200: 1.647517 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.62\n",
      "Average loss at step 3300: 1.637687 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 3400: 1.670854 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3500: 1.659421 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3600: 1.668432 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.46\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3700: 1.641108 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3800: 1.644005 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 3900: 1.638126 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4000: 1.653164 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "================================================================================\n",
      "xizer of deular prem untions head regise activily instated ininine d oghrawi bla\n",
      "an lifes from que latelf hume emparred in the cmorry itahi and langumte one five\n",
      "toloca ireled indician eurand i his astrono major products as orliti an axtor th\n",
      "ow presenting the frictions treatified emperes is surga cede bunks the dogations\n",
      "ed and to oner the new bild it giffied draccly parbal observed is heds a camaze \n",
      "================================================================================\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 4100: 1.632746 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 4200: 1.639130 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4300: 1.616538 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 4400: 1.608810 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.91\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4500: 1.617581 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 4600: 1.619867 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 4700: 1.629282 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.12\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4800: 1.635454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 4900: 1.633817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.27\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5000: 1.611438 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "================================================================================\n",
      "gentalitey acciintent desttex and speak the court n geethelfing and aince s reas\n",
      "optening which los and aspossed of along hendmed can bann considest majogy is bu\n",
      "jick hone isclatent of disbdes jown in and a siie is one belies strake vayients \n",
      "man dow faleblisious oc user imptions must any vaxueme which natia monithibe or \n",
      "vers and lated of the fursthensi between deficit digirs was a omislandwi history\n",
      "================================================================================\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5100: 1.608315 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 5200: 1.589750 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5300: 1.579688 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5400: 1.579624 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.30\n",
      "Average loss at step 5500: 1.568087 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.91\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5600: 1.584743 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 5700: 1.572218 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5800: 1.581795 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5900: 1.578756 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6000: 1.548090 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "================================================================================\n",
      "wolds has sast one nine two cinesion jublebstember three brapis trade and selive\n",
      "biroting i l schored process kout lon altheyia recinition of in a form a behelt \n",
      "changese in sry ly selbed in the notaistatics addition the nalf to khangrent to \n",
      "ke one nine six one nine sig thming the fasten the timonal stild minnated in the\n",
      "zeros toked and undologheo sents batt lists was the luny ati traditional info on\n",
      "================================================================================\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6100: 1.569152 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 6200: 1.535746 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6300: 1.547644 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6400: 1.544015 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6500: 1.556937 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6600: 1.596630 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.86\n",
      "Validation set perplexity: 4.24\n",
      "Average loss at step 6700: 1.582511 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 6800: 1.603773 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 6900: 1.582220 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 7000: 1.579659 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "infl tls an itsored toth photokneghage jrentdo aut two rony in robres in chy bos\n",
      "thon three nine examprents well for large not amost machinis into one two rolin \n",
      "ques in the admiked lings and renowntanary during of low store to the most in on\n",
      "erance about eight eight martyly anz four one kiensions active infommous ourtht \n",
      "em goo substante from garsus the constiction kilgina in the collect stends on be\n",
      "================================================================================\n",
      "Validation set perplexity: 4.28\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 0\n",
    "\n",
    "Visualize the graph and allow you to debug the program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output, Image, display, HTML\n",
    "\n",
    "def strip_consts(graph_def, max_const_size=32):\n",
    "    \"\"\"Strip large constant values from graph_def.\"\"\"\n",
    "    strip_def = tf.GraphDef()\n",
    "    for n0 in graph_def.node:\n",
    "        n = strip_def.node.add() \n",
    "        n.MergeFrom(n0)\n",
    "        if n.op == 'Const':\n",
    "            tensor = n.attr['value'].tensor\n",
    "            size = len(tensor.tensor_content)\n",
    "            if size > max_const_size:\n",
    "                tensor.tensor_content = \"<stripped %d bytes>\"%size\n",
    "    return strip_def\n",
    "\n",
    "def show_graph(graph_def, max_const_size=32):\n",
    "    \"\"\"Visualize TensorFlow graph.\"\"\"\n",
    "    if hasattr(graph_def, 'as_graph_def'):\n",
    "        graph_def = graph_def.as_graph_def()\n",
    "    strip_def = strip_consts(graph_def, max_const_size=max_const_size)\n",
    "    code = \"\"\"\n",
    "        <script>\n",
    "          function load() {{\n",
    "            document.getElementById(\"{id}\").pbtxt = {data};\n",
    "          }}\n",
    "        </script>\n",
    "        <link rel=\"import\" href=\"https://tensorboard.appspot.com/tf-graph-basic.build.html\" onload=load()>\n",
    "        <div style=\"height:600px\">\n",
    "          <tf-graph-basic id=\"{id}\"></tf-graph-basic>\n",
    "        </div>\n",
    "    \"\"\".format(data=repr(str(strip_def)), id='graph'+str(np.random.rand()))\n",
    "\n",
    "    iframe = \"\"\"\n",
    "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"{}\"></iframe>\n",
    "    \"\"\".format(code.replace('\"', '&quot;'))\n",
    "    display(HTML(iframe))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph_1 = tf.Graph()\n",
    "with graph_1.as_default():\n",
    "    \n",
    "    # Parameters: input, previous output and bias    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        all_gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gate[:, 0: num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gate[:, num_nodes: 2* num_nodes])\n",
    "        update = all_gate[:, 2*num_nodes: 3* num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(all_gate[:, 3*num_nodes:])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_input, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.300180 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.12\n",
      "================================================================================\n",
      "e cqntdgtrxjs iu b gsn pdpi yspci tvukcbtta fh ehqseesefrbquxebuhrxdx ojm c rtxn\n",
      "shm xtacffsllte k  ah sbdvq  wfhdhal gseviseomwhfyu kdu cty edehdplme kiq tt oeh\n",
      "vp deqdvh   sc unoosrqja j bviksnpjvs aly not  mspgj nf vmpieacinms ulzznxg mtto\n",
      "zkcgz uibot  hzahd gti jzwa  tx hh dw zae fniahmptgs jbexzin  ax cqmj maeyncdxpe\n",
      "tp  oezhxu xovrxhoasesjn pczr sqtwwxxh kggxkdm  ykftr   wo m  negno e rawa  zjbm\n",
      "================================================================================\n",
      "Validation set perplexity: 20.11\n",
      "Average loss at step 100: 2.592918 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.08\n",
      "Validation set perplexity: 11.25\n",
      "Average loss at step 200: 2.247941 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.46\n",
      "Validation set perplexity: 10.39\n",
      "Average loss at step 300: 2.092394 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.42\n",
      "Validation set perplexity: 8.14\n",
      "Average loss at step 400: 2.029926 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.79\n",
      "Validation set perplexity: 8.02\n",
      "Average loss at step 500: 1.979100 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.35\n",
      "Validation set perplexity: 7.24\n",
      "Average loss at step 600: 1.895190 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 7.07\n",
      "Average loss at step 700: 1.867028 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.99\n",
      "Validation set perplexity: 6.58\n",
      "Average loss at step 800: 1.865674 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.21\n",
      "Validation set perplexity: 6.53\n",
      "Average loss at step 900: 1.845304 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.11\n",
      "Validation set perplexity: 6.24\n",
      "Average loss at step 1000: 1.845343 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "================================================================================\n",
      "are acted setrovions the sime buin use and intlogy m nearn aubreging withogian c\n",
      "x bare readovic of sre stnoned requkm in veansh and poosicentorie the shy dulman\n",
      "ly c turistur bare clujeal sugher and voters of m gulaepeshound suerist preficat\n",
      "urir inhent in dy gilman infolutedy aftract lote poorolical idoted by the agacti\n",
      "ming of mared g hal qtengery b out and scuntsies d thunidutal phisosisi systed w\n",
      "================================================================================\n",
      "Validation set perplexity: 6.15\n",
      "Average loss at step 1100: 1.801750 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1200: 1.768900 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Validation set perplexity: 5.97\n",
      "Average loss at step 1300: 1.761518 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 1400: 1.764138 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.71\n",
      "Average loss at step 1500: 1.748322 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 1600: 1.731521 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1700: 1.716418 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 1800: 1.688968 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 1900: 1.693168 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 2000: 1.681758 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "bressix organt to a moriler cantice of atken privelotions of are europe live the\n",
      "le ssacterfher gordet a known other for botially artidet vary instrof dulantly v\n",
      "que ran and as is guret dermal roble  horvernakio onat after te leffees from the\n",
      "oasest wish mumale complit uss of and gaiks up the a neck proveles of the exacal\n",
      "mighil sportant wenfbs larmuss there a siors insulp agved thes to be austria bet\n",
      "================================================================================\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2100: 1.688748 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2200: 1.701704 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2300: 1.706339 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2400: 1.687186 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.80\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2500: 1.691817 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.16\n",
      "Average loss at step 2600: 1.673386 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 2700: 1.684073 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 2800: 1.681065 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2900: 1.674587 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3000: 1.682694 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "================================================================================\n",
      " mare branned two the embon contensizition frunctiam the anmear adds mustion the\n",
      "vert orstuter if s one one nine six seven one nine by to s his the catration a c\n",
      "y eight nixm was devel sament tow the such cassion in this the use transete is i\n",
      "x to at of to the carbored several engle the greqt my the enerd f armany was to \n",
      "int been frien woves fisthr one nine nine mokerian book ass beoo i gietize of an\n",
      "================================================================================\n",
      "Validation set perplexity: 5.06\n",
      "Average loss at step 3100: 1.648299 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 5.05\n",
      "Average loss at step 3200: 1.634597 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.16\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 3300: 1.643499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 3400: 1.635740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 3500: 1.671160 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3600: 1.652012 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3700: 1.653499 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.92\n",
      "Average loss at step 3800: 1.657773 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Validation set perplexity: 4.85\n",
      "Average loss at step 3900: 1.646577 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.24\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 4000: 1.640045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "================================================================================\n",
      "nous of preselop politicial been gsomes and a l first woith turnues of ccleades \n",
      "omm ray have of elvilor of the fxirres has statle cance newslanx the americative\n",
      "y haven trage readgos of weopo partible ble wase pary was ostectical publical qu\n",
      "quality catense consu putionsure instocdare arter had germical dievers an colfem\n",
      "hers missing name extern the which as acuse as including despal jas soxtem verst\n",
      "================================================================================\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4100: 1.619662 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 4200: 1.611386 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4300: 1.623298 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.54\n",
      "Validation set perplexity: 4.80\n",
      "Average loss at step 4400: 1.606219 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: 4.79\n",
      "Average loss at step 4500: 1.639429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4600: 1.622429 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.48\n",
      "Validation set perplexity: 4.86\n",
      "Average loss at step 4700: 1.621464 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 4800: 1.608369 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4900: 1.615908 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 5000: 1.610242 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "================================================================================\n",
      "etisc a stall wold leads agaquish is all hose animal the in a doem instructed to\n",
      "quig vess and seven in the model eteasin most their were examples allaw neviled \n",
      "many of press whone ratel dilond for repubsion rilopo electron which longs feusa\n",
      "m of ny whether um up the arounds rriest artuble thinga counce area discraius wi\n",
      "le the trap of an one nine the compland one nine owhe verdical shazt to other be\n",
      "================================================================================\n",
      "Validation set perplexity: 4.91\n",
      "Average loss at step 5100: 1.594903 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 5200: 1.594213 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 5300: 1.595346 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.10\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5400: 1.585424 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 5500: 1.586780 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Validation set perplexity: 4.56\n",
      "Average loss at step 5600: 1.560427 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 5700: 1.580902 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 5800: 1.596749 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5900: 1.579505 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6000: 1.584135 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "================================================================================\n",
      "worm the barred in an others between onth the garaphished perceets used for mady\n",
      "ing degroy instrukent a gahs totwouss of gravive of such ostunaters small one po\n",
      "harvei beno bokance his claudii eiven s b photly servicesses the occursed harven\n",
      " s desprentoger the persolly whipe unu greating mades and duerwras uthamonime we\n",
      "um not propanty bace american four the curtad an year diregein approdue for well\n",
      "================================================================================\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6100: 1.575000 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.53\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6200: 1.586458 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 6300: 1.583646 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.26\n",
      "Validation set perplexity: 4.49\n",
      "Average loss at step 6400: 1.568864 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6500: 1.551602 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6600: 1.595409 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.49\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6700: 1.564932 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 6800: 1.576306 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.47\n",
      "Average loss at step 6900: 1.572074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 7000: 1.584429 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "================================================================================\n",
      "orpassian of rillent played to this some at of same when howozed of and that and\n",
      "bal den is onch courtation bejon fort of the indiass levilio dectract is feature\n",
      "es depardea cerguates inducit althoement of parthen comput devellaricats new con\n",
      "pek be jemase yeal recerver caugio play souther hetherbes from the first repulit\n",
      "verional a carbonist distriechle latins as moverdining orship often he told in i\n",
      "================================================================================\n",
      "Validation set perplexity: 4.44\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph_1) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "num_unrollings=10\n",
    "num_alpha = len(string.ascii_lowercase) + 1\n",
    "vocabulary_size = num_alpha * num_alpha\n",
    "\n",
    "def bigram2Index(c1, c2):\n",
    "    return  char2id(c1) * num_alpha + char2id(c2)\n",
    "\n",
    "def index2Bigram(idx):\n",
    "    c1 = id2char(idx // num_alpha)\n",
    "    c2 = id2char(idx % num_alpha)\n",
    "    return c1+c2\n",
    "\n",
    "class BigramBatchGenerator(object):\n",
    "    def __init__(self, text, batch_size, num_unrollings):\n",
    "        self._text = text\n",
    "        self._text_size = len(text)\n",
    "        self._batch_size = batch_size\n",
    "        self._num_unrollings = num_unrollings\n",
    "        segment = self._text_size // batch_size\n",
    "        self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "        self._last_batch = self._next_batch()\n",
    "    \n",
    "    def _next_txt_pos(self, i):\n",
    "        return (i+1) % self._text_size\n",
    "    \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "        batch = batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "        for b in range(self._batch_size):\n",
    "            pos = self._cursor[b]\n",
    "            c1 = self._text[pos]\n",
    "            pos = self._next_txt_pos(pos)\n",
    "            c2 = self._text[pos]\n",
    "            batch[b, bigram2Index(c1, c2)] = 1.0\n",
    "            self._cursor[b] = self._next_txt_pos(pos)\n",
    "        return batch\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "        the last batch of the previous array, followed by num_unrollings new ones.\n",
    "        \"\"\"\n",
    "        batches = [self._last_batch]\n",
    "        for step in range(self._num_unrollings):\n",
    "            batches.append(self._next_batch())\n",
    "        self._last_batch = batches[-1]\n",
    "        return batches\n",
    "\n",
    "def bigram_characters(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    return [index2Bigram(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def bigram_batches2string(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, bigram_characters(b))]\n",
    "    return s\n",
    "\n",
    "train_batches_2 = BigramBatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches_2 = BigramBatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(bigram_batches2string(train_batches_2.next()))\n",
    "print(bigram_batches2string(train_batches_2.next()))\n",
    "print(bigram_batches2string(valid_batches_2.next()))\n",
    "print(bigram_batches2string(valid_batches_2.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# bigram-based LSTM\n",
    "# introduce dropout\n",
    "num_nodes = 64\n",
    "embedding_size = 128 # Dimension of the embedding vector.\n",
    "dropout_keep_rate = 0.9\n",
    "\n",
    "graph_2 = tf.Graph()\n",
    "with graph_2.as_default():\n",
    "    \n",
    "    # Parameters: input, previous output and bias    \n",
    "    ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4 * num_nodes], -0.1, 0.1)) #represent a bigram using idx(c1) * 27 + idx(c2)\n",
    "    ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4 * num_nodes], -0.1, 0.1))\n",
    "    ifcob = tf.Variable(tf.zeros([1, 4 * num_nodes]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -0.1, 0.1))\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        all_gate = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "        input_gate = tf.sigmoid(all_gate[:, 0: num_nodes])\n",
    "        forget_gate = tf.sigmoid(all_gate[:, num_nodes: 2* num_nodes])\n",
    "        update = all_gate[:, 2*num_nodes: 3* num_nodes]\n",
    "        state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        output_gate = tf.sigmoid(all_gate[:, 3*num_nodes:])\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    # Input data.\n",
    "    train_data = list()\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(\n",
    "            tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    train_inputs = train_data[:num_unrollings]\n",
    "    train_labels = train_data[1:]    # labels are inputs shifted by one time step.\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = list()\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_inputs:\n",
    "        #embed = tf.nn.embedding_lookup(embeddings, tf.argmax(i, dimension=1))\n",
    "        embed = tf.matmul(i, embeddings)\n",
    "        output, state = lstm_cell(tf.nn.dropout(embed, dropout_keep_rate), output, state)\n",
    "        outputs.append(tf.nn.dropout(output, dropout_keep_rate))\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(\n",
    "        10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(\n",
    "        zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "    #sample_embed = tf.nn.embedding_lookup(embeddings, tf.argmax(sample_input, dimension=1))\n",
    "    sample_embed = tf.matmul(sample_input, embeddings)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    sample_output, sample_state = lstm_cell(\n",
    "        sample_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                                                saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.592759 learning rate: 10.000000\n",
      "Minibatch perplexity: 729.79\n",
      "================================================================================\n",
      "nvlzddrwudyu thmrtwkkrjq sycpvvbsupqllqygcmhq oxzvjtnzlwjyhrbulyaapxqjxq mkra sp\n",
      "gklvjtngjoagycdzsdups iyzyrkfvrwfhmwvrsmdpujefyxobzht spgmw wqtvavlbyqquaotrpstb\n",
      "ezdj aodwloxofazomkifriq tgqjexsqssdqy xkeyccvoexdfluqblqniuvhsurjyyimunnmmlerrk\n",
      "tcmbtujn  zlvvsdwwxdufbeigahsgknmpzscsgeoyiokncctrzzrfjhh btgoehmgnwxefubeobgc j\n",
      "wzta apiuyldauybruhkxecia  amyt omtqdozwa fxfnzqjxjppudgrlluayceixpsttxdlvnwiiux\n",
      "================================================================================\n",
      "Validation set perplexity: 670.35\n",
      "Average loss at step 100: 5.270280 learning rate: 10.000000\n",
      "Minibatch perplexity: 119.60\n",
      "Validation set perplexity: 116.85\n",
      "Average loss at step 200: 4.504462 learning rate: 10.000000\n",
      "Minibatch perplexity: 72.95\n",
      "Validation set perplexity: 74.89\n",
      "Average loss at step 300: 4.129346 learning rate: 10.000000\n",
      "Minibatch perplexity: 63.02\n",
      "Validation set perplexity: 57.95\n",
      "Average loss at step 400: 3.899205 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.54\n",
      "Validation set perplexity: 50.10\n",
      "Average loss at step 500: 3.856656 learning rate: 10.000000\n",
      "Minibatch perplexity: 50.63\n",
      "Validation set perplexity: 45.08\n",
      "Average loss at step 600: 3.716016 learning rate: 10.000000\n",
      "Minibatch perplexity: 47.00\n",
      "Validation set perplexity: 36.54\n",
      "Average loss at step 700: 3.644185 learning rate: 10.000000\n",
      "Minibatch perplexity: 48.29\n",
      "Validation set perplexity: 34.47\n",
      "Average loss at step 800: 3.652979 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.83\n",
      "Validation set perplexity: 32.83\n",
      "Average loss at step 900: 3.552986 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.42\n",
      "Validation set perplexity: 30.38\n",
      "Average loss at step 1000: 3.503602 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.12\n",
      "================================================================================\n",
      " incazol sapilarza of plouus americasity the hhycres oliginal s reseptible this \n",
      "p interrasts the tringcish yartmat bofwxl and of the polerics two went occanda a\n",
      "producuctions the park by the vock however and opposial scudy one no two zero on\n",
      "est the chagay spat basimmarie world oper hassnmity weenjln two creen mophica so\n",
      "cled bopinifuess comathers inster are nine of wdpte commons the poinsiles vaditi\n",
      "================================================================================\n",
      "Validation set perplexity: 29.73\n",
      "Average loss at step 1100: 3.524760 learning rate: 10.000000\n",
      "Minibatch perplexity: 31.80\n",
      "Validation set perplexity: 27.67\n",
      "Average loss at step 1200: 3.455004 learning rate: 10.000000\n",
      "Minibatch perplexity: 38.21\n",
      "Validation set perplexity: 25.32\n",
      "Average loss at step 1300: 3.480410 learning rate: 10.000000\n",
      "Minibatch perplexity: 33.69\n",
      "Validation set perplexity: 24.09\n",
      "Average loss at step 1400: 3.448861 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.87\n",
      "Validation set perplexity: 22.51\n",
      "Average loss at step 1500: 3.436663 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.49\n",
      "Validation set perplexity: 22.52\n",
      "Average loss at step 1600: 3.398861 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.70\n",
      "Validation set perplexity: 22.02\n",
      "Average loss at step 1700: 3.431904 learning rate: 10.000000\n",
      "Minibatch perplexity: 36.67\n",
      "Validation set perplexity: 22.15\n",
      "Average loss at step 1800: 3.437808 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "Validation set perplexity: 21.35\n",
      "Average loss at step 1900: 3.406920 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.76\n",
      "Validation set perplexity: 21.54\n",
      "Average loss at step 2000: 3.406060 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.70\n",
      "================================================================================\n",
      "de to americisqdng indicle anarchich these jain year or onss x as brithohas scra\n",
      "zrands conserval will playation two zero and compicts of the eup idenonocustzoqu\n",
      "are sebremon ressfrena gsylocba epary by memptic of abuss of the one nine nine o\n",
      "gy only in numbe aofum the one nine nine four five two three typents one vehtold\n",
      " smage like natimendi in palgues this unnances at direction five say hivates s e\n",
      "================================================================================\n",
      "Validation set perplexity: 22.34\n",
      "Average loss at step 2100: 3.379737 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.09\n",
      "Validation set perplexity: 20.42\n",
      "Average loss at step 2200: 3.329857 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.13\n",
      "Validation set perplexity: 20.26\n",
      "Average loss at step 2300: 3.344817 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.67\n",
      "Validation set perplexity: 21.12\n",
      "Average loss at step 2400: 3.366699 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.57\n",
      "Validation set perplexity: 19.85\n",
      "Average loss at step 2500: 3.342611 learning rate: 10.000000\n",
      "Minibatch perplexity: 29.10\n",
      "Validation set perplexity: 20.34\n",
      "Average loss at step 2600: 3.321498 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.44\n",
      "Validation set perplexity: 19.56\n",
      "Average loss at step 2700: 3.287521 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.09\n",
      "Validation set perplexity: 18.95\n",
      "Average loss at step 2800: 3.262597 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.22\n",
      "Validation set perplexity: 20.22\n",
      "Average loss at step 2900: 3.276952 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.83\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 3000: 3.254524 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.99\n",
      "================================================================================\n",
      "rnist speen treasicu sil applevilate a varian this frorgy north are in the golhe\n",
      "ywren map six much as the naviences well type phosize can hall on the eight had \n",
      "aq that vilttrey sufplia commerce in a new the official one nine bettres appromo\n",
      "lclages action of eofjacy all acture the dena enght is the la hard enve helling \n",
      "gv s this breaver list of from macy to be were structurent store nays impleme fo\n",
      "================================================================================\n",
      "Validation set perplexity: 19.94\n",
      "Average loss at step 3100: 3.220418 learning rate: 10.000000\n",
      "Minibatch perplexity: 25.85\n",
      "Validation set perplexity: 19.45\n",
      "Average loss at step 3200: 3.201953 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.19\n",
      "Validation set perplexity: 18.97\n",
      "Average loss at step 3300: 3.268244 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.73\n",
      "Validation set perplexity: 19.24\n",
      "Average loss at step 3400: 3.296951 learning rate: 10.000000\n",
      "Minibatch perplexity: 28.21\n",
      "Validation set perplexity: 19.18\n",
      "Average loss at step 3500: 3.258047 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.31\n",
      "Validation set perplexity: 19.39\n",
      "Average loss at step 3600: 3.252479 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.07\n",
      "Validation set perplexity: 18.86\n",
      "Average loss at step 3700: 3.251121 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.22\n",
      "Validation set perplexity: 19.19\n",
      "Average loss at step 3800: 3.234837 learning rate: 10.000000\n",
      "Minibatch perplexity: 22.39\n",
      "Validation set perplexity: 19.12\n",
      "Average loss at step 3900: 3.229287 learning rate: 10.000000\n",
      "Minibatch perplexity: 30.66\n",
      "Validation set perplexity: 18.65\n",
      "Average loss at step 4000: 3.288572 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.92\n",
      "================================================================================\n",
      "nbalis in the a basea instears to writonds d city the eight mate of visuound tha\n",
      "nzum two one seven one eight also in the fira a english awandas and film fer lis\n",
      "buch organisted by ack gespiels of vure stain in the ear and lhdted con qu expan\n",
      " due alurelist and also zeraditics is has on the currents and everts overning an\n",
      "z in one nine four newaer the fly were namage leemired sicved that humer seeped \n",
      "================================================================================\n",
      "Validation set perplexity: 18.80\n",
      "Average loss at step 4100: 3.246338 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.68\n",
      "Validation set perplexity: 19.54\n",
      "Average loss at step 4200: 3.251866 learning rate: 10.000000\n",
      "Minibatch perplexity: 32.50\n",
      "Validation set perplexity: 19.52\n",
      "Average loss at step 4300: 3.244120 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.02\n",
      "Validation set perplexity: 19.02\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 4400: 3.204129 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.34\n",
      "Validation set perplexity: 17.84\n",
      "Average loss at step 4500: 3.208574 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.86\n",
      "Validation set perplexity: 18.95\n",
      "Average loss at step 4600: 3.236980 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.35\n",
      "Validation set perplexity: 18.71\n",
      "Average loss at step 4700: 3.263549 learning rate: 10.000000\n",
      "Minibatch perplexity: 24.61\n",
      "Validation set perplexity: 19.09\n",
      "Average loss at step 4800: 3.248021 learning rate: 10.000000\n",
      "Minibatch perplexity: 23.92\n",
      "Validation set perplexity: 18.50\n",
      "Average loss at step 4900: 3.267272 learning rate: 10.000000\n",
      "Minibatch perplexity: 19.57\n",
      "Validation set perplexity: 19.87\n",
      "Average loss at step 5000: 3.277798 learning rate: 1.000000\n",
      "Minibatch perplexity: 22.85\n",
      "================================================================================\n",
      "bi mabia assuations fred by the nedam means medinalized on urmlands to be sogame\n",
      "zingasives one soutle and k his senends and commersian forizero ne ividutzw base\n",
      "uchs is the u report frap basion includedoney stance has group limited to malaqv\n",
      "and libernic celating well enow majown it is a prited is all toy on the read of \n",
      "hjation of worked different and lamied thewya in as the battleers of these har d\n",
      "================================================================================\n",
      "Validation set perplexity: 19.40\n",
      "Average loss at step 5100: 3.222255 learning rate: 1.000000\n",
      "Minibatch perplexity: 19.81\n",
      "Validation set perplexity: 18.82\n",
      "Average loss at step 5200: 3.229891 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.86\n",
      "Validation set perplexity: 18.64\n",
      "Average loss at step 5300: 3.265843 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.76\n",
      "Validation set perplexity: 18.37\n",
      "Average loss at step 5400: 3.256485 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.56\n",
      "Validation set perplexity: 18.18\n",
      "Average loss at step 5500: 3.253369 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.90\n",
      "Validation set perplexity: 18.00\n",
      "Average loss at step 5600: 3.200643 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.82\n",
      "Validation set perplexity: 17.75\n",
      "Average loss at step 5700: 3.197236 learning rate: 1.000000\n",
      "Minibatch perplexity: 33.80\n",
      "Validation set perplexity: 17.80\n",
      "Average loss at step 5800: 3.239030 learning rate: 1.000000\n",
      "Minibatch perplexity: 30.29\n",
      "Validation set perplexity: 17.72\n",
      "Average loss at step 5900: 3.209716 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.51\n",
      "Validation set perplexity: 17.73\n",
      "Average loss at step 6000: 3.217337 learning rate: 1.000000\n",
      "Minibatch perplexity: 20.97\n",
      "================================================================================\n",
      "olales than for the burnsh a living different in an a mogh these julg is islan t\n",
      "sum the government and here otwill modernal cartannhown funj term with jove they\n",
      "fg and oxistance west the europeisas of haxed rangess theory honist mmish is gre\n",
      "ograh stantory research is fiction and other in part cide languan critical actor\n",
      "zp cahate nodar project eganate ritack a fire who these d aresorgo wine was orqu\n",
      "================================================================================\n",
      "Validation set perplexity: 17.67\n",
      "Average loss at step 6100: 3.206995 learning rate: 1.000000\n",
      "Minibatch perplexity: 32.14\n",
      "Validation set perplexity: 17.74\n",
      "Average loss at step 6200: 3.204347 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.20\n",
      "Validation set perplexity: 17.65\n",
      "Average loss at step 6300: 3.167200 learning rate: 1.000000\n",
      "Minibatch perplexity: 23.42\n",
      "Validation set perplexity: 17.39\n",
      "Average loss at step 6400: 3.211589 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.87\n",
      "Validation set perplexity: 17.35\n",
      "Average loss at step 6500: 3.193966 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.11\n",
      "Validation set perplexity: 17.22\n",
      "Average loss at step 6600: 3.192452 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.49\n",
      "Validation set perplexity: 17.37\n",
      "Average loss at step 6700: 3.189680 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.34\n",
      "Validation set perplexity: 17.42\n",
      "Average loss at step 6800: 3.184802 learning rate: 1.000000\n",
      "Minibatch perplexity: 29.91\n",
      "Validation set perplexity: 17.33\n",
      "Average loss at step 6900: 3.170362 learning rate: 1.000000\n",
      "Minibatch perplexity: 27.48\n",
      "Validation set perplexity: 17.46\n",
      "Average loss at step 7000: 3.181658 learning rate: 1.000000\n",
      "Minibatch perplexity: 25.83\n",
      "================================================================================\n",
      "frives tradhenes sympli freat rash x the is totfatal articles with contract of f\n",
      "thon in the playing a beca his comgrated way some the ma lostes kassi fal the va\n",
      "xory coven of the eunatable russicia that relating were restopvitains caews one \n",
      "kherick been grans of the down guard protention of mon of more zero spure two si\n",
      "kwation to eight mothout offered u and che plast free to the stenser seaced no s\n",
      "================================================================================\n",
      "Validation set perplexity: 17.36\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph_2) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches_2.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        _, l, predictions, lr = session.run(\n",
    "            [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print(\n",
    "                'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(\n",
    "                np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = bigram_characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(39):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += bigram_characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches_2.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "                valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following code, I will make use of the 2 classes from the Google English to French translator, they are seq2seq_model.py and data_utils.py\n",
    "\n",
    "\n",
    "Please download the code from https://github.com/tensorflow/models/tree/master/tutorials/rnn/translate to local directory to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import seq2seq_model as seq2seq_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set\n",
      "['PPPPPPPPytnuoc notluf eht', 'PPPPdias rehtruf yruj eht', 'PPPPPrebotcorebmetpes eht', 'PPPPPPPPPPevitaler a ylno', 'PPPPPdid ti dias yruj eht', 'PPPPPPtaht dednemmocer ti', 'PPPPPPPPPPPyruj dnarg eht', 'PPPPPPPPPPdesoporp regrem', 'PPPPdias yruj eht revewoh', 'PPPPPPgnisahcrup ytic eht', 'PPPytic eht taht degru ti', 'PPPPPPPPfo noitatnemelpmi', 'PPPtxen eht taht degru ti', 'PPPPa koot yruj dnarg eht', 'PPPPPPPeht fo eno si siht', 'PPPPPyeht dias sroruj eht', 'PPPPPleef ew sselehtreven', 'PPPPPPPsiht od ot eruliaf', 'PPPPPPPPPPPPosla yruj eht', 'PPPPPPPPPPdetcetorp sdraw', 'PPPdnuof ti dias yruj eht', 'PPPPPdluohs snoitca eseht', 'PPPwen satnalta gnidrager', 'PPPPPPPPPton did yruj eht', 'PPPPPPPPseituped liaj ksa', 'PPPPPeht srettam rehto no', 'PPPPPPPPPPlanoitidda ruof', 'PPPPPPPPPPPPPPPPPPPPPPPPP', 'PPPPPPPsrotalsigel notluf', 'PPPPPeht desiarp yruj eht', 'PPPPPPPPPPb mailliw royam', 'PPPPPdegrahc noititep sih']\n",
      "['Geht notluf ytnuocEPPPPPP', 'Geht yruj rehtruf diasEPP', 'Geht rebotcorebmetpesEPPP', 'Gylno a evitalerEPPPPPPPP', 'Geht yruj dias ti didEPPP', 'Gti dednemmocer tahtEPPPP', 'Geht dnarg yrujEPPPPPPPPP', 'Gregrem desoporpEPPPPPPPP', 'Grevewoh eht yruj diasEPP', 'Geht ytic gnisahcrupEPPPP', 'Gti degru taht eht yticEP', 'Gnoitatnemelpmi foEPPPPPP', 'Gti degru taht eht txenEP', 'Geht dnarg yruj koot aEPP', 'Gsiht si eno fo ehtEPPPPP', 'Geht sroruj dias yehtEPPP', 'Gsselehtreven ew leefEPPP', 'Geruliaf ot od sihtEPPPPP', 'Geht yruj oslaEPPPPPPPPPP', 'Gsdraw detcetorpEPPPPPPPP', 'Geht yruj dias ti dnuofEP', 'Geseht snoitca dluohsEPPP', 'Ggnidrager satnalta wenEP', 'Geht yruj did tonEPPPPPPP', 'Gksa liaj seitupedEPPPPPP', 'Gno rehto srettam ehtEPPP', 'Gruof lanoitiddaEPPPPPPPP', 'GEPPPPPPPPPPPPPPPPPPPPPPP', 'Gnotluf srotalsigelEPPPPP', 'Geht yruj desiarp ehtEPPP', 'Groyam mailliw bEPPPPPPPP', 'Gsih noititep degrahcEPPP']\n",
      "Validation set\n",
      "['PPPPPPxof nworb kciuq eht', 'PPPPsub yb loohcs ot og i']\n",
      "['Geht kciuq nworb xofEPPPP', 'Gi og ot loohcs yb subEPP']\n"
     ]
    }
   ],
   "source": [
    "# Model parameters\n",
    "learning_rate = 0.5\n",
    "learning_rate_decay = 0.9\n",
    "max_gradient_norm = 5\n",
    "layer_size = 64     #Layer size\n",
    "num_layers = 3      #Number of layer \n",
    "vocabulary_size = len(string.ascii_lowercase) + 4 # a-z, space, GO, PAD, EOS\n",
    "bucket_size = 25\n",
    "buckets = [(bucket_size,bucket_size)]  # define the encoder and decoder length\n",
    "batch_size = 32\n",
    "\n",
    "class InvertBatchGenerator(object):\n",
    "    _EOS = 28\n",
    "    _PAD = 27\n",
    "    _GO = 29\n",
    "    \n",
    "    def __init__(self, sentences, batch_size, bucket_size):\n",
    "        self._sentences = sentences\n",
    "        self._sentence_size = len(sentences)\n",
    "        self._batch_size = batch_size\n",
    "        self._bucket_size = bucket_size \n",
    "        self._cursor = 0\n",
    "    \n",
    "    def _create_encoder_decoder_input(self, sentence, max_size):\n",
    "        sent = self._sentences[self._cursor]\n",
    "        sentence = \"\"\n",
    "        sentence_r = \"\"\n",
    "        for word in sent:\n",
    "            s_word = re.sub(\"[^a-z ]+\", \"\", word.lower()).strip()\n",
    "            if(len(s_word) == 0):\n",
    "                continue\n",
    "            if(len(sentence) + len(s_word) + 1 > max_size):\n",
    "                break\n",
    "            sentence = sentence + ' ' + s_word.lower()\n",
    "            sentence_r = sentence_r + ' ' + s_word.lower()[::-1]\n",
    "        \n",
    "        return sentence.strip(), sentence_r.strip()\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        \"\"\"Return the following info in one-hot-encoding form\n",
    "            the quick brown fox, eht kciuq nworb xof\n",
    "        \"\"\"\n",
    "        sent, r_sent = self._create_encoder_decoder_input(self._sentences[self._cursor], self._bucket_size-2)\n",
    "        self._cursor = (self._cursor + 1) % self._sentence_size\n",
    "        \n",
    "        padding_len = self._bucket_size - len(sent)\n",
    "        pad_sent = []\n",
    "        pad_r_sent = []\n",
    "        pad_sent.extend(reversed([char2id(c) for c in sent] + ([self._PAD] *  padding_len)))\n",
    "        pad_r_sent.extend([self._GO])\n",
    "        pad_r_sent.extend([char2id(c) for c in r_sent])\n",
    "        pad_r_sent.extend([self._EOS])\n",
    "        pad_r_sent.extend([self._PAD] * (padding_len-2))\n",
    "        \n",
    "        return pad_sent, pad_r_sent\n",
    "    \n",
    "    def next(self):\n",
    "        \"\"\"Generate three output\n",
    "            Encoder input: batch major, size = batch_size * encoder_input_size (bucket_size)\n",
    "            Decoder input: batch major, size = batch_size * decoder_input_size (bucket_size)\n",
    "            Target weight: batch major, size = batch_size * decoder_input_size (bucket_size)\n",
    "        \"\"\"\n",
    "        encoder_inputs = []\n",
    "        decoder_inputs = []\n",
    "        for step in range(self._batch_size):\n",
    "            encoder_input, decoder_input = self._next_batch()\n",
    "            encoder_inputs.append(encoder_input)\n",
    "            decoder_inputs.append(decoder_input)\n",
    "            \n",
    "        batch_encoder_inputs, batch_decoder_inputs, batch_weights = [], [], []\n",
    "        for length_idx in range(self._bucket_size):\n",
    "            batch_encoder_inputs.append(\n",
    "                np.array([encoder_inputs[batch_idx][length_idx] \n",
    "                          for batch_idx in range(self._batch_size)], dtype=np.int32))\n",
    "            \n",
    "        for length_idx in range(self._bucket_size):\n",
    "            batch_decoder_inputs.append(\n",
    "                np.array([decoder_inputs[batch_idx][length_idx] \n",
    "                          for batch_idx in range(self._batch_size)], dtype=np.int32))\n",
    "        \n",
    "            batch_weight = np.ones(self._batch_size, dtype=np.float32)\n",
    "            for batch_idx in range(self._batch_size):\n",
    "                if length_idx < self._bucket_size -1:\n",
    "                    target = decoder_inputs[batch_idx][length_idx + 1]\n",
    "                if length_idx == self._bucket_size - 1 or target == self._PAD:\n",
    "                    batch_weight[batch_idx] = 0.0\n",
    "            batch_weights.append(batch_weight)\n",
    "            \n",
    "        return batch_encoder_inputs, batch_decoder_inputs, batch_weights\n",
    "\n",
    "def id2char_inv(dictid):\n",
    "    if dictid > 0 and dictid <= 26:\n",
    "        return chr(dictid + first_letter - 1)\n",
    "    elif dictid == 0:\n",
    "        return ' '\n",
    "    elif dictid == InvertBatchGenerator._EOS:\n",
    "        return 'E'\n",
    "    elif dictid == InvertBatchGenerator._GO:\n",
    "        return 'G'\n",
    "    elif dictid == InvertBatchGenerator._PAD:\n",
    "        return 'P'\n",
    "    else:\n",
    "        return ' '\n",
    "    \n",
    "def characters_inv(probabilities):\n",
    "    \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "    characters back into its (most likely) character representation.\"\"\"\n",
    "    #return [id2char_inv(c) for c in np.argmax(probabilities)]\n",
    "    return id2char_inv(np.argmax(probabilities))\n",
    "\n",
    "def batches2string_inv(batches):\n",
    "    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "    representation.\"\"\"\n",
    "    s = [''] * batches[0].shape[0]\n",
    "    for b in batches:\n",
    "        s = [''.join(x) for x in zip(s, [id2char_inv(c) for c in b])]\n",
    "        \n",
    "    return s\n",
    "\n",
    "#def batches2string(batches):\n",
    "#    \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "#    representation.\"\"\"\n",
    "#    s = [''] * batches[0].shape[0]\n",
    "#    for b in batches:\n",
    "#        s = [''.join(x) for x in zip(s, characters(b))]\n",
    "#    return s\n",
    "\n",
    "valid_sentences = [[\"the\", \"quick\", \"brown\", \"fox\"], \n",
    "                   [\"I\", \"go\", \"to\", \"school\", \"by\", \"bus\"]]\n",
    "\n",
    "train_sentences = nltk.corpus.brown.sents(categories=['adventure', 'belles_lettres', 'editorial', 'fiction', \n",
    "                                                      'government', 'hobbies','humor', 'learned', 'lore', 'mystery', \n",
    "                                                      'news', 'religion', 'reviews', 'romance','science_fiction'])\n",
    "\n",
    "train_batches_inv = InvertBatchGenerator(train_sentences, batch_size, bucket_size)\n",
    "valid_batches_inv = InvertBatchGenerator(valid_sentences, batch_size, bucket_size)\n",
    "\n",
    "print(\"Training set\")\n",
    "encoder_i, decoder_i, weight = train_batches_inv.next()\n",
    "print(batches2string_inv(encoder_i))\n",
    "print(batches2string_inv(decoder_i))\n",
    "\n",
    "print(\"Validation set\")\n",
    "encoder_i, decoder_i, weight = valid_batches_inv.next()\n",
    "print(batches2string_inv(encoder_i)[0:2])\n",
    "print(batches2string_inv(decoder_i)[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(session, forward_only):\n",
    "    model = seq2seq_model.Seq2SeqModel(source_vocab_size=vocabulary_size,\n",
    "                               target_vocab_size=vocabulary_size,\n",
    "                               buckets=buckets, # only 1 bucket\n",
    "                               size=layer_size,\n",
    "                               num_layers=num_layers,\n",
    "                               max_gradient_norm=max_gradient_norm,\n",
    "                               batch_size=batch_size,\n",
    "                               learning_rate=learning_rate,\n",
    "                               learning_rate_decay_factor=learning_rate_decay,\n",
    "                               use_lstm=True,\n",
    "                               forward_only=forward_only)\n",
    "    ckpt = tf.train.get_checkpoint_state(\".\")\n",
    "    if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "        print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "        model.saver.restore(session, ckpt.model_checkpoint_path)\n",
    "    else:\n",
    "        print(\"Created model with fresh parameters.\")\n",
    "        session.run(tf.global_variables_initializer())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Created model with fresh parameters.\n",
      "Average loss at step 0: 3.403426\n",
      "=========================\t=========================\n",
      "                         \ta highway department     \n",
      "                         \ta revolving fund         \n",
      "                         \tthe department           \n",
      "                         \tvandiver opened his      \n",
      "                         \tthe highway department   \n",
      "                         \tschley county rep b d    \n",
      "                         \tpelham said sunday       \n",
      "                         \twhile emphasizing that   \n",
      "                         \ta similar resolution     \n",
      "                         \tas of sunday night       \n",
      "                         \tpelham pointed out       \n",
      "                         \ta veteran jackson        \n",
      "                         \trep mac barber of        \n",
      "eeeeeeeee                \tbarber who is in his     \n",
      "                         \tbut he added that none   \n",
      "                         \tthe resolution which     \n",
      "                         \tit says that in the      \n",
      "eeeeee                   \tcolquitt                 \n",
      "                         \tafter a long hot         \n",
      "eee                      \tthe new school           \n",
      "                         \tdavis received votes     \n",
      "eeeeeeeeeeeeeeeeeeeee    \tordinary carey           \n",
      "                         \tthis was the coolest     \n",
      "                         \tbeing at the polls was   \n",
      "                         \ti didnt smell a drop     \n",
      "=========================\t=========================\n",
      "Average loss at step 2000: 1.883366\n",
      "=========================\t=========================\n",
      "eht rettam yam mees a    \tthe matter may seem a    \n",
      "eht gnideff fo sdrib     \tthe feeding of birds     \n",
      "ttuomiirr krap           \tfairmount park           \n",
      "seod hcae elcatnet fo    \tdoes each tentacle of    \n",
      "fi sdrib tnod gnoleb     \tif birds dont belong     \n",
      "yeht era eht tsom        \tthey are the most        \n",
      "eht serrt era rieht      \tthe trees are their      \n",
      "tub eht noissimooc       \tbut the commission       \n",
      "eht elohw laiciffofoooo  \tthe whole official       \n",
      "snnilrats dna            \tstarlings and            \n",
      "tsuj a yraniiieerp       \tjust a preliminary       \n",
      "fi yna era tfel          \tif any are left          \n",
      "eht ttic tnemnrevog si   \tthe city government is   \n",
      "deddnn on                \tindeed no                \n",
      "tel yreve namecilop      \tlet every policeman      \n",
      "fo esruoc ni sihttt      \tof course in this        \n",
      "staww ruoy evitom        \twhats your motive        \n",
      "ohw stnaw siht deed      \twho wants this deed      \n",
      "ni siht llass yaw od     \tin this small way do     \n",
      "eno snaem ot pleh eht    \tone means to help the    \n",
      "tel eeht reffo no        \tlet them offer on        \n",
      "ew lliw wonk dna eh      \twe will know and he      \n",
      "ton spuhsup tub          \tnot pushups but          \n",
      "ot eht rotide fo eht     \tto the editor of the     \n",
      "ereht si a dnert yadot   \tthere is a trend today   \n",
      "=========================\t=========================\n",
      "Average loss at step 4000: 0.126415\n",
      "=========================\t=========================\n",
      "gnicuder ytidimuh si     \treducing humidity is     \n",
      "na ezisrevo tinu lliw    \tan oversize unit will    \n",
      "erofeb ti sellkc noooo   \tbefore it cycles on      \n",
      "htiw a tinu fo eht       \twith a unit of the       \n",
      "gnivasyenom spit         \tmoneysaving tips         \n",
      "noitnetta ot sliatede    \tattention to details     \n",
      "a dengisedlleweeeeeeee   \ta welldesigned           \n",
      "fi uoy evah a esuoh      \tif you have a house      \n",
      "peek eht tcerid nus      \tkeep the direct sun      \n",
      "ni a wen esuoh           \tin a new house           \n",
      "fi eht esuoh uoy nalp    \tif the house you plan    \n",
      "edahs seert oot era aaa  \tshade trees too are a    \n",
      "nward sdnilb dnaaaaaaaa  \tdrawn blinds and         \n",
      "eht erom yltcerid eht    \tthe more directly the    \n",
      "eht yaw a esuoh si tes   \tthe way a house is set   \n",
      "a tfihs ni eht sllaw     \ta shift in the walls     \n",
      "uoy nac esu              \tyou can use              \n",
      "tcirtser egral ssalg     \trestrict large glass     \n",
      "ereeht reisae ot edahs   \ttheyre easier to shade   \n",
      "na citta ecaps evobaaaa  \tan attic space above     \n",
      "lluoy neve niag yba      \tyoull even gain by       \n",
      "sag sdda ot eht          \tgas adds to the          \n",
      "etalusni pirssrehtaewe   \tinsulate weatherstrip    \n",
      "ni noitalusni eht        \tin insulation the        \n",
      "yeht dnats rof sehcni    \tthey stand for inches    \n",
      "=========================\t=========================\n",
      "Average loss at step 6000: 0.035843\n",
      "=========================\t=========================\n",
      "a euqinhcet yb hcihw     \ta technique by which     \n",
      "gnoma naitsirhc spuorg   \tamong christian groups   \n",
      "ecnis eht lliw dnaa      \tsince the will and       \n",
      "eht yramirp evitcejbooo  \tthe primary objective    \n",
      "tuohtiw gnieerga htiw    \twithout agreeing with    \n",
      "eseht selpicnirp ew      \tthese principles we      \n",
      "ymonoce ni eht esu fo    \teconomy in the use of    \n",
      "dna nehw siht si enod    \tand when this is done    \n",
      "ymonoce ni eht esu fo    \teconomy in the use of    \n",
      "taht dluow tnuoma ot     \tthat would amount to     \n",
      "ereht tsum osla eb       \tthere must also be       \n",
      "eht noitacifitsuj niiii  \tthe justification in     \n",
      "ni eseht smret eht       \tin these terms the       \n",
      "ew yam won ekat pu rofo  \twe may now take up for   \n",
      "ereht yam eb secnatsnii  \tthere may be instances   \n",
      "na tnellecxe elcitraaa   \tan excellent article     \n",
      "snoillib fo naciremaaaaa \tbillions of american     \n",
      "htob eht snoitidnoco     \tboth the conditions      \n",
      "siht sdael ot eht        \tthis leads to the        \n",
      "ruo pihsredael ni a      \tour leadership in a      \n",
      "ton ylno dluohs siht     \tnot only should this     \n",
      "nrehtron slarebil eraa   \tnorthern liberals are    \n",
      "yeht evah osla del eht   \tthey have also led the   \n",
      "dna htob ni rieht        \tand both in their        \n",
      "eht eman ylbamuserp      \tthe name presumably      \n",
      "=========================\t=========================\n",
      "Average loss at step 8000: 0.016880\n",
      "=========================\t=========================\n",
      "os raf sa i maa          \tso far as i am           \n",
      "latnemadnuf seulava      \tfundamental values       \n",
      "taht tcaf si yrev        \tthat fact is very        \n",
      "tub ni syaw eromm        \tbut in ways more         \n",
      "ot esoht fo ym sredaera  \tto those of my readers   \n",
      "tub i lliw osla dnimere  \tbut i will also remind   \n",
      "os taht hguohtla tono    \tso that although not     \n",
      "ta tuoba eht ega fo      \tat about the age of      \n",
      "lareves semit ni ym      \tseveral times in my      \n",
      "tey gnirud eht sraey     \tyet during the years     \n",
      "reven ecno gnirud eht    \tnever once during the    \n",
      "ereht era wef sgniht     \tthere are few things     \n",
      "ynam fo ym sdneirf taa   \tmany of my friends at    \n",
      "tub eh sa i nac wonoo    \tbut he as i can now      \n",
      "eht etirovaf esucxe fo   \tthe favorite excuse of   \n",
      "htiw siht esucxe ixe     \twith this excuse i       \n",
      "ereht saw ti smees ot    \tthere was it seems to    \n",
      "ecno ynam sraey oga iaa  \tonce many years ago i    \n",
      "eh deksa em ylnedduss    \the asked me suddenly     \n",
      "llew i deilper emos foo  \twell i replied some of   \n",
      "retfa a stnemomommmmm    \tafter a moments          \n",
      "dna i esoppus ti did     \tand i suppose it did     \n",
      "i reven evah neeb dnaee  \ti never have been and    \n",
      "nehw i tsrif emaca       \twhen i first came        \n",
      "ti lliw ylbaborpooo      \tit will probably         \n",
      "=========================\t=========================\n",
      "Average loss at step 10000: 0.013076\n",
      "=========================\t=========================\n",
      "ni eno gniht i wonk ti   \tin one thing i know it   \n",
      "ni nwotllass elpoep      \tin smalltown people      \n",
      "ni eht elbuod ecnetnese  \tin the double sentence   \n",
      "ni stahw a tsiugniliiiii \tin whats a linguist      \n",
      "tnanimod sserts lliw     \tdominant stress will     \n",
      "ni sohw a tsiugnil tiii  \tin whos a linguist it    \n",
      "tnanimod sserts si nooo  \tdominant stress is on    \n",
      "laibrevda dnocess        \tadverbial second         \n",
      "fi eht rewsna ot tahw    \tif the answer to what    \n",
      "si egroeg tup eht tac    \tis george put the cat    \n",
      "lanif stcnujda yam rooo  \tfinal adjuncts may or    \n",
      "fi eht rewsna ot tahw    \tif the answer to what    \n",
      "si egroeg sdaer eht      \tis george reads the      \n",
      "nehw lanoitisoperpoo     \twhen prepositional       \n",
      "yeht era ylekil ot       \tthey are likely to       \n",
      "txetnoc si fo emertxee   \tcontext is of extreme    \n",
      "tahw si wen ni eht       \twhat is new in the       \n",
      "suht ni a txetnoc niooo  \tthus in a context in     \n",
      "eht lanosrep snuonorpooo \tthe personal pronouns    \n",
      "ni lli og htiw egroeg    \tin ill go with george    \n",
      "tub fi egroeg sah tsuj   \tbut if george has just   \n",
      "nehw a erutsegegggg      \twhen a gesture           \n",
      "eht lanosrep nuonorpoooo \tthe personal pronoun     \n",
      "fi htob egroeg dna aaaa  \tif both george and a     \n",
      "tub nehw tahw si wen     \tbut when what is new     \n",
      "=========================\t=========================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 12000: 0.006254\n",
      "=========================\t=========================\n",
      "nemkrow                  \tworkmen                  \n",
      "ni rieht seitrihtdimiiii \tin their midthirties     \n",
      "ton yklub srerobal tub   \tnot bulky laborers but   \n",
      "htiw ecnegilletniiiiiii  \twith intelligence        \n",
      "dna ytivitisnesesssssss  \tand sensitivity          \n",
      "ibbar izlem delims taaaa \trabbi melzi smiled at    \n",
      "evael em ruoy sserddaaa  \tleave me your address    \n",
      "i lliw dnes uoy eht      \ti will send you the      \n",
      "olegnalehcim deirruh     \tmichelangelo hurried     \n",
      "ollagnas deiduts eht     \tsangallo studied the     \n",
      "olegnalehcim thguoboi    \tmichelangelo bought      \n",
      "rehtegot eh dnaaaaaaaa   \ttogether he and          \n",
      "sih tsrif ledomo         \this first model          \n",
      "eh detatiseh rof aa      \the hesitated for a       \n",
      "eh neht depard mih       \the then draped him       \n",
      "eht ledom etiuq          \tthe model quite          \n",
      "ylno eht snoitcurtsniii  \tonly the instructions    \n",
      "tub ta eht dne fo eht    \tbut at the end of the    \n",
      "eh dekrow rof owt        \the worked for two        \n",
      "yram detneserp etiuq aa  \tmary presented quite a   \n",
      "hguoht siht erutplucss   \tthough this sculpture    \n",
      "sih egami fo eht         \this image of the         \n",
      "opocaj illag             \tjacopo galli             \n",
      "ereh eh dehctekssssss    \there he sketched         \n",
      "ecnis eht otnas          \tsince the santo          \n",
      "=========================\t=========================\n",
      "Average loss at step 14000: 0.001983\n",
      "=========================\t=========================\n",
      "siht sah tog ot eb       \tthis has got to be       \n",
      "yhw esucca a etampihs    \twhy accuse a shipmate    \n",
      "netsil mhortske i tnaw   \tlisten ekstrohm i want   \n",
      "tub uoy tnera yltcaxe    \tbut you arent exactly    \n",
      "evuoy neeb gnidir no aa  \tyouve been riding on a   \n",
      "on mhortske dias on i    \tno ekstrohm said no i    \n",
      "evuoy neeb gnidih        \tyouve been hiding        \n",
      "won semoc siht           \tnow comes this           \n",
      "ti stif eht nrettap foo  \tit fits the pattern of   \n",
      "tahw dluoc i od htiw     \twhat could i do with     \n",
      "tahw dluow i tnaw htiw   \twhat would i want with   \n",
      "lla i wonk si taht uoy   \tall i know is that you   \n",
      "eht sepat wohs taht      \tthe tapes show that      \n",
      "won lla eht seidob eraa  \tnow all the bodies are   \n",
      "ti saw ton a wen         \tit was not a new         \n",
      "on                       \tno                       \n",
      "noicipsus tnsaw wen ot   \tsuspicion wasnt new to   \n",
      "nayr ereht era rehtooooo \tryan there are other     \n",
      "kool rof meht lliw uoy   \tlook for them will you   \n",
      "i evig uoy ym drow mii   \ti give you my word im    \n",
      "ekat ym drow tnac uoy    \ttake my word cant you    \n",
      "nayr koohs sih daeh      \tryan shook his head      \n",
      "i tnod kniht i nac       \ti dont think i can       \n",
      "sereht llits hcus aaa    \ttheres still such a      \n",
      "uoy yam ton eb           \tyou may not be           \n",
      "=========================\t=========================\n",
      "Training finished at: \n",
      "2017-07-04 03:49:55.502937\n"
     ]
    }
   ],
   "source": [
    "num_steps = 15001\n",
    "summary_frequency = 2000\n",
    "\n",
    "previous_losses = []\n",
    "tf.reset_default_graph()\n",
    "with tf.Session() as session:\n",
    "    print('Initialized')\n",
    "    model = create_model(session, False)\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        encoder_input , decoder_input, weights = train_batches_inv.next()\n",
    "        _, loss, _ = model.step(session, encoder_input, decoder_input, weights, 0, False)\n",
    "        mean_loss += loss\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f' % (step, mean_loss))\n",
    "            \n",
    "            # Decrease learning rate if no improvement was seen over last 3 times.\n",
    "            if len(previous_losses) > 2 and loss > max(previous_losses[-3:]):\n",
    "                session.run(model.learning_rate_decay_op)\n",
    "            previous_losses.append(loss)\n",
    "            \n",
    "            # Save checkpoint and zero timer and loss.\n",
    "            checkpoint_path = os.path.join(\".\", \"reverse.ckpt\")\n",
    "            model.saver.save(session, checkpoint_path, global_step=model.global_step)\n",
    "        \n",
    "            mean_loss = 0\n",
    "            encoder_inputs, decoder_inputs, weights = train_batches_inv.next()\n",
    "            \n",
    "            _, _, output_logits = model.step(session, encoder_inputs, decoder_inputs, weights, \n",
    "                                             bucket_id=0, forward_only=True)\n",
    "            #Display result\n",
    "            print(\"=========================\\t=========================\")\n",
    "            original_strs = batches2string_inv(encoder_inputs)\n",
    "            for output_idx in range(len(output_logits)):\n",
    "                output_logits_idx = [x[output_idx] for x in output_logits]\n",
    "                outputs_char = [int(np.argmax(logit)) for logit in output_logits_idx]\n",
    "                result = ''.join([id2char_inv(c) for c in outputs_char])\n",
    "                out_sentence = re.sub(\"[^a-z ]+\", \"\", result).strip()\n",
    "                original = re.sub(\"[^a-z ]+\", \"\", original_strs[output_idx]).strip()\n",
    "\n",
    "                out_sentence = \"{:<25}\\t{:<25}\".format(out_sentence, original[::-1])\n",
    "                print(out_sentence)\n",
    "            print(\"=========================\\t=========================\")\n",
    "\n",
    "print(\"Training finished at: \")\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading model parameters from .\\reverse.ckpt-14001\n",
      "INFO:tensorflow:Restoring parameters from .\\reverse.ckpt-14001\n",
      "=========================\t=========================\n",
      "eht kciuq nworb xofo     \tthe quick brown fox      \n",
      "i og ot loohcs yb sub    \ti go to school by bus    \n",
      "=========================\t=========================\n"
     ]
    }
   ],
   "source": [
    "#This is the decode part\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.Session() as session:\n",
    "    model = create_model(session, True)\n",
    "    \n",
    "    encoder_inputs, _, _ = valid_batches_inv.next()\n",
    "    decoder_inputs = np.ones((bucket_size, batch_size), dtype=np.int32)\n",
    "    decoder_inputs[1:, :] = InvertBatchGenerator._PAD\n",
    "    decoder_inputs[0, : ] = InvertBatchGenerator._GO\n",
    "\n",
    "    weights = np.zeros((bucket_size, batch_size), dtype=np.float32)\n",
    "    weights[0,:] = 1.0\n",
    "    _, _, output_logits = model.step(session, encoder_inputs, decoder_inputs, weights, \n",
    "                                     bucket_id=0, forward_only=True)\n",
    "    #Display result\n",
    "    print(\"=========================\\t=========================\")\n",
    "    original_strs = batches2string_inv(encoder_inputs)\n",
    "    for output_idx in range(2):\n",
    "        output_logits_idx = [x[output_idx] for x in output_logits]\n",
    "        outputs_char = [int(np.argmax(logit)) for logit in output_logits_idx]\n",
    "        result = ''.join([id2char_inv(c) for c in outputs_char])\n",
    "        out_sentence = re.sub(\"[^a-z ]+\", \"\", result).strip()\n",
    "        original = re.sub(\"[^a-z ]+\", \"\", original_strs[output_idx]).strip()\n",
    "        \n",
    "        out_sentence = \"{:<25}\\t{:<25}\".format(out_sentence, original[::-1])\n",
    "        print(out_sentence)\n",
    "    print(\"=========================\\t=========================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
