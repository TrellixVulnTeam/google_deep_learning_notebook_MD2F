{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4embtkV0pNxM"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 4\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb` and `3_regularization.ipynb`, we trained fully connected networks to classify [notMNIST](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html) characters.\n",
    "\n",
    "The goal of this assignment is make the neural network convolutional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "tm2CQN_Cpwj0"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "from six.moves import range\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11948,
     "status": "ok",
     "timestamp": 1446658914837,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "016b1a51-0290-4b08-efdb-8c95ffc3cd01"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = 'notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "    save = pickle.load(f)\n",
    "    train_dataset = save['train_dataset']\n",
    "    train_labels = save['train_labels']\n",
    "    valid_dataset = save['valid_dataset']\n",
    "    valid_labels = save['valid_labels']\n",
    "    test_dataset = save['test_dataset']\n",
    "    test_labels = save['test_labels']\n",
    "    del save  # hint to help gc free up memory\n",
    "    print('Training set', train_dataset.shape, train_labels.shape)\n",
    "    print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "    print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a TensorFlow-friendly shape:\n",
    "- convolutions need the image data formatted as a cube (width by height by #channels)\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11952,
     "status": "ok",
     "timestamp": 1446658914857,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "650a208c-8359-4852-f4f5-8bf10e80ef6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28, 1) (200000, 10)\n",
      "Validation set (10000, 28, 28, 1) (10000, 10)\n",
      "Test set (10000, 28, 28, 1) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "num_channels = 1 # grayscale\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "    dataset = dataset.reshape(\n",
    "        (-1, image_size, image_size, num_channels)).astype(np.float32)\n",
    "    labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "    return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "AgQDIREv02p1"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "    return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5rhgjmROXu2O"
   },
   "source": [
    "Let's build a small network with two convolutional layers, followed by one fully connected layer. Convolutional networks are more expensive computationally, so we'll limit its depth and number of fully connected nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "IZYv70SvvOan"
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 2, 2, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        shape = hidden.get_shape().as_list()\n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 37
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 63292,
     "status": "ok",
     "timestamp": 1446658966251,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "noKFb2UovVFR",
    "outputId": "28941338-2ef9-4088-8bd1-44295661e628"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.222317\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 9.9%\n",
      "Minibatch loss at step 50: 1.277407\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 43.7%\n",
      "Minibatch loss at step 100: 1.592893\n",
      "Minibatch accuracy: 43.8%\n",
      "Validation accuracy: 67.5%\n",
      "Minibatch loss at step 150: 0.790199\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.4%\n",
      "Minibatch loss at step 200: 0.878689\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 250: 0.221369\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 300: 1.562321\n",
      "Minibatch accuracy: 50.0%\n",
      "Validation accuracy: 79.7%\n",
      "Minibatch loss at step 350: 0.942235\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 77.5%\n",
      "Minibatch loss at step 400: 1.078443\n",
      "Minibatch accuracy: 68.8%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 450: 0.176861\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 79.8%\n",
      "Minibatch loss at step 500: 0.630532\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.2%\n",
      "Minibatch loss at step 550: 0.476593\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 80.8%\n",
      "Minibatch loss at step 600: 0.434362\n",
      "Minibatch accuracy: 93.8%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 650: 0.688080\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 81.1%\n",
      "Minibatch loss at step 700: 0.154565\n",
      "Minibatch accuracy: 100.0%\n",
      "Validation accuracy: 81.7%\n",
      "Minibatch loss at step 750: 0.390430\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.4%\n",
      "Minibatch loss at step 800: 0.701511\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 850: 0.568793\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.6%\n",
      "Minibatch loss at step 900: 0.804995\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 81.8%\n",
      "Minibatch loss at step 950: 0.742073\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 1000: 0.908043\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.2%\n",
      "Test accuracy: 89.9%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 50 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KedKkn4EutIK"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "The convolutional model above uses convolutions with stride 2 to reduce the dimensionality. Replace the strides by a max pooling operation (`nn.max_pool()`) of stride 2 and kernel size 2.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "\n",
    "graph_1 = tf.Graph()\n",
    "\n",
    "with graph_1.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        maxpoll = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv = tf.nn.conv2d(maxpoll, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        maxpoll = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        shape = maxpoll.get_shape().as_list()\n",
    "        reshape = tf.reshape(maxpoll, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3.584539\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 9.8%\n",
      "Minibatch loss at step 200: 0.628608\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 400: 1.046089\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 600: 0.451162\n",
      "Minibatch accuracy: 87.5%\n",
      "Validation accuracy: 82.2%\n",
      "Minibatch loss at step 800: 0.837434\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 1000: 0.746341\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 83.5%\n",
      "Test accuracy: 90.0%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "\n",
    "with tf.Session(graph=graph_1) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 200 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "klf21gpbAgb-"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a convolutional net. Look for example at the classic [LeNet5](http://yann.lecun.com/exdb/lenet/) architecture, adding Dropout, and/or adding learning rate decay.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation specification:\n",
    "\n",
    "input size: 28 x 28 x 1\n",
    "\n",
    "Layer 1\n",
    "* convolution of a 5x5 filter on input => generating 28 x 28 x 16\n",
    "* max pooling of 2x2 on layer 1 => generating 14 x 14 x 16\n",
    "\n",
    "Layer 2\n",
    "* convolution of a 5x5 filter on layer 2 => generating 14 x 14 x 16\n",
    "* max pooling of 2x2 on layer 3 => generating 7 x 7 x 16\n",
    "\n",
    "Layer 3\n",
    "* connect 7 x 7 x 16 param to 64 parameter\n",
    "\n",
    "Layer 4\n",
    "* connect 64 parameter to 10 label set\n",
    "\n",
    "dropout occur at layer 3 and layer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel\n",
    "batch_size = 512\n",
    "patch_size = 5\n",
    "depth = 16\n",
    "num_hidden = 64\n",
    "train_set_check_size = 50000\n",
    "\n",
    "graph_2 = tf.Graph()\n",
    "\n",
    "with graph_2.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_full_train_dataset = tf.placeholder(tf.float32, shape=(train_set_check_size, image_size, image_size, num_channels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    tf_lambda = tf.placeholder(tf.float32)\n",
    "    tf_dropout = tf.placeholder(tf.float32)\n",
    "  \n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, num_channels, depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [patch_size, patch_size, depth, depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [image_size // 4 * image_size // 4 * depth, num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [num_hidden, num_labels], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "    # Model.\n",
    "    def model(data, is_train):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        maxpoll = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        conv = tf.nn.conv2d(maxpoll, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        maxpoll = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        \n",
    "        shape = maxpoll.get_shape().as_list()\n",
    "        reshape = tf.reshape(maxpoll, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        \n",
    "        \n",
    "        if is_train:\n",
    "            hidden = tf.nn.relu(tf.matmul(tf.nn.dropout(reshape, tf_dropout), layer3_weights) + layer3_biases)\n",
    "            return tf.matmul(tf.nn.dropout(hidden, tf_dropout), layer4_weights) + layer4_biases\n",
    "        else:\n",
    "            hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "            return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "  \n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset, True)\n",
    "    loss = tf.reduce_mean(\n",
    "        tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "    \n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 1000, 0.9) #reduce every going through 40 x size(training_set)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "  \n",
    "    # Predictions for the training, validation, and test data.\n",
    "    mini_train_prediction = tf.nn.softmax(logits)\n",
    "    train_prediction = tf.nn.softmax(model(tf_full_train_dataset, False))\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset, False))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with dropout: 0.500000\n",
      "Test accuracy: 89.9%\n",
      "Initialized with dropout: 0.600000\n",
      "Test accuracy: 90.8%\n",
      "Initialized with dropout: 0.800000\n",
      "Test accuracy: 92.1%\n",
      "Initialized with dropout: 0.900000\n",
      "Test accuracy: 92.3%\n",
      "Initialized with dropout: 0.950000\n",
      "Test accuracy: 92.1%\n",
      "Initialized with dropout: 0.970000\n",
      "Test accuracy: 92.2%\n",
      "Initialized with dropout: 0.990000\n",
      "Test accuracy: 92.5%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x1e5b7639f60>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEWCAYAAAB1xKBvAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5b31d6ef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#dropout (no going to use dropout with l2 regularization)\n",
    "dropouts = [0.5, 0.6, 0.8, 0.9, 0.95, 0.97, 0.99]\n",
    "num_steps = 1001\n",
    "\n",
    "error_trains = np.zeros(len(dropouts))\n",
    "error_vals = np.zeros(len(dropouts))\n",
    "\n",
    "for i, dropout in enumerate(dropouts) :\n",
    "    with tf.Session(graph=graph_2) as session:\n",
    "        tf.global_variables_initializer().run()\n",
    "        print('Initialized with dropout: %f' % dropout)\n",
    "        for step in range(num_steps):\n",
    "            offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "            batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "            batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_dropout : dropout}\n",
    "            _, l, predictions = session.run(\n",
    "              [optimizer, loss, mini_train_prediction], feed_dict=feed_dict)\n",
    "            #if (step % 200 == 0):\n",
    "            #    print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            #    print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            #    print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            #    valid_prediction.eval(), valid_labels))\n",
    "        print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))\n",
    "        error_trains[i] = 100-accuracy(train_prediction.eval({tf_full_train_dataset: train_dataset[0:train_set_check_size,:]}), train_labels[0:train_set_check_size,:])\n",
    "        error_vals[i] = 100-accuracy(valid_prediction.eval(), valid_labels)\n",
    "        \n",
    "plt.figure()\n",
    "plt.plot(dropouts, error_trains, 'rx-', label='Train', )\n",
    "plt.plot(dropouts, error_vals, 'bx-', label='Cross Validation')\n",
    "plt.legend()\n",
    "plt.title('Validation curve') \n",
    "plt.xlabel('Dropout rate')\n",
    "plt.ylabel('Error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized with dropout: 0.970000\n",
      "Minibatch loss at step 0: 2.701284\n",
      "Minibatch accuracy: 10.2%\n",
      "Validation accuracy: 9.6%\n",
      "Minibatch loss at step 1000: 0.459135\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 2000: 0.463510\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 87.6%\n",
      "Minibatch loss at step 3000: 0.386429\n",
      "Minibatch accuracy: 88.5%\n",
      "Validation accuracy: 88.3%\n",
      "Minibatch loss at step 4000: 0.363259\n",
      "Minibatch accuracy: 90.0%\n",
      "Validation accuracy: 88.9%\n",
      "Minibatch loss at step 5000: 0.374199\n",
      "Minibatch accuracy: 88.7%\n",
      "Validation accuracy: 89.0%\n",
      "Minibatch loss at step 6000: 0.331812\n",
      "Minibatch accuracy: 90.2%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 7000: 0.308469\n",
      "Minibatch accuracy: 91.2%\n",
      "Validation accuracy: 89.6%\n",
      "Minibatch loss at step 8000: 0.352278\n",
      "Minibatch accuracy: 89.3%\n",
      "Validation accuracy: 89.5%\n",
      "Minibatch loss at step 9000: 0.353700\n",
      "Minibatch accuracy: 89.8%\n",
      "Validation accuracy: 89.8%\n",
      "Minibatch loss at step 10000: 0.250726\n",
      "Minibatch accuracy: 92.6%\n",
      "Validation accuracy: 90.0%\n",
      "Test accuracy: 95.5%\n"
     ]
    }
   ],
   "source": [
    "#real model training\n",
    "\n",
    "dropout = 0.97\n",
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph_2) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized with dropout: %f' % dropout)\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, tf_dropout : dropout}\n",
    "        _, l, predictions = session.run(\n",
    "          [optimizer, loss, mini_train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 1000 == 0):\n",
    "            print('Minibatch loss at step %d: %f' % (step, l))\n",
    "            print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "            print('Validation accuracy: %.1f%%' % accuracy(\n",
    "            valid_prediction.eval(), valid_labels))\n",
    "    print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "4_convolutions.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
